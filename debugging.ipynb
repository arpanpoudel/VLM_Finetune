{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66847333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import ChestDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3291cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= ChestDataset(index_path='index_files/mimic_index_frontal_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985cdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "min_pixels =224*224\n",
    "max_pixels = 512*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375e2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Qwen2_5_VLProcessor.from_pretrained(model_id, use_fast=True, min_pixels=min_pixels, max_pixels=max_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ffe152",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.padding_side='right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79fd851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenDataCollator:\n",
    "    def __init__(self, processor, max_length=512):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.system_message =\"\"\"You are an expert radiologist trained in interpreting chest X-rays. Given radiographic image of the chest (frontal view), generate a detailed and clinically accurate radiology report. The report should include a summary of the observed findings and, if possible, an impression that highlights key diagnoses or abnormalities. Do not speculate beyond the image content. Use professional radiological language. Maintain a neutral, factual tone suitable for inclusion in a patient medical record. Do NOT reference previous imaging or mention paging physicians. Instead, describe the chest x-ray in a stand-alone format.\"\"\"\n",
    "    def format_data(self, sample):\n",
    "        return [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\" : [{\"type\": \"text\", \"text\": self.system_message}]\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": sample[\"image\"],\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\":[\n",
    "                    {\"type\": \"text\", \"text\": sample[\"text\"]},\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        # Extract images and texts from the batch\n",
    "        examples = [self.format_data(sample) for sample in examples]\n",
    "        texts =[\n",
    "        self.processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "        ]\n",
    "        image_inputs = [process_vision_info(sample)[0] for sample in examples]\n",
    "        # Tokenize the texts and process the images\n",
    "        batch = self.processor(\n",
    "            text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "        #extract only assistant content\n",
    "        input_ids_lists = batch['input_ids'].tolist()\n",
    "        assert len(examples) == len(input_ids_lists)\n",
    "        labels_list = []\n",
    "        for ids_list in input_ids_lists:\n",
    "            label_ids = [-100] * len(ids_list)\n",
    "            for begin_end_indexs in find_assistant_content_sublist_indexes(ids_list):\n",
    "                label_ids[begin_end_indexs[0]:begin_end_indexs[1]] = ids_list[begin_end_indexs[0]:begin_end_indexs[1]]\n",
    "            labels_list.append(label_ids)\n",
    "        \n",
    "        labels = torch.tensor(labels_list, dtype= torch.int64)\n",
    "        batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "        return batch\n",
    " \n",
    "def find_assistant_content_sublist_indexes(l):\n",
    "    '''\n",
    "    This function tries to find the indexes of the assistant content in the input_ids list to build labels.\n",
    "    '''\n",
    "    # (Pdb++) processor.tokenizer.encode(\"<|im_start|>assistant\\n\")\n",
    "    # [151644, 77091, 198]\n",
    "    # (Pdb++) processor.tokenizer.encode(\"<|im_end|>\\n\")\n",
    "    # [151645, 198]\n",
    "\n",
    "    start_indexes = []\n",
    "    end_indexes = []\n",
    "\n",
    "    # Iterate through the list to find starting points\n",
    "    for i in range(len(l) - 2):\n",
    "        # Check if the current and next elements form the start sequence\n",
    "        if l[i] == 151644 and l[i+1] == 77091 and l[i+2] == 198:\n",
    "            start_indexes.append(i+3)\n",
    "            # Now look for the first 151645 and 198 after the start\n",
    "            for j in range(i+3, len(l)-1):\n",
    "                if l[j] == 151645 and l[j+1] == 198:\n",
    "                    end_indexes.append(j+2) # **NOTE** the <|im_end|>\\n 2 tokens should be included in the label, so that model can predicate end of output.\n",
    "                    break  # Move to the next start after finding the end\n",
    "\n",
    "    return list(zip(start_indexes, end_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af0b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch =[dataset[0], dataset[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aced7047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': <PIL.Image.Image image mode=RGB size=2539x3050>,\n",
       "  'text': 'FINDINGS:\\nNone\\n\\nIMPRESSION:\\nThere is less bilateral pleural effusion, predominantly on the left. The cardiac\\nsilhouette and bilateral basal parenchymal opacities appear stable. No evidence\\nof pneumonia or pulmonary edema.'},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=2022x2022>,\n",
       "  'text': 'FINDINGS:\\nNone\\n\\nIMPRESSION:\\nThe dual-channel ICD device with leads in the right atrium and apex of the right ventricle is visible without any apparent abnormalities. The cardiac silhouette is within normal limits, and there is no evidence of vascular congestion, pleural effusion, or acute focal pneumonia.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94f0122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenDataCollator:\n",
    "    def __init__(self, processor, max_length=512):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.system_message =\"\"\"You are an expert radiologist trained in interpreting chest X-rays. Given radiographic image of the chest (frontal view), generate a detailed and clinically accurate radiology report. The report should include a summary of the observed findings and, if possible, an impression that highlights key diagnoses or abnormalities. Do not speculate beyond the image content. Use professional radiological language. Maintain a neutral, factual tone suitable for inclusion in a patient medical record. Do NOT reference previous imaging or mention paging physicians. Instead, describe the chest x-ray in a stand-alone format.\"\"\"\n",
    "    def format_data(self, sample):\n",
    "        return [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\" : [{\"type\": \"text\", \"text\": self.system_message}]\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": sample[\"image\"],\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\":[\n",
    "                    {\"type\": \"text\", \"text\": sample[\"text\"]},\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        # Extract images and texts from the batch\n",
    "        examples = [self.format_data(sample) for sample in examples]\n",
    "        texts =[\n",
    "        self.processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "        ]\n",
    "        image_inputs = [process_vision_info(sample)[0] for sample in examples]\n",
    "        # Tokenize the texts and process the images\n",
    "        batch = self.processor(\n",
    "            text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "        #extract only assistant content\n",
    "        input_ids_lists = batch['input_ids'].tolist()\n",
    "        assert len(examples) == len(input_ids_lists)\n",
    "        labels_list = []\n",
    "        for ids_list in input_ids_lists:\n",
    "            label_ids = [-100] * len(ids_list)\n",
    "            for begin_end_indexs in find_assistant_content_sublist_indexes(ids_list):\n",
    "                label_ids[begin_end_indexs[0]:begin_end_indexs[1]] = ids_list[begin_end_indexs[0]:begin_end_indexs[1]]\n",
    "            labels_list.append(label_ids)\n",
    "        \n",
    "        labels = torch.tensor(labels_list, dtype= torch.int64)\n",
    "        batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "        return batch\n",
    " \n",
    "def find_assistant_content_sublist_indexes(l):\n",
    "    '''\n",
    "    This function tries to find the indexes of the assistant content in the input_ids list to build labels.\n",
    "    '''\n",
    "    # (Pdb++) processor.tokenizer.encode(\"<|im_start|>assistant\\n\")\n",
    "    # [151644, 77091, 198]\n",
    "    # (Pdb++) processor.tokenizer.encode(\"<|im_end|>\\n\")\n",
    "    # [151645, 198]\n",
    "\n",
    "    start_indexes = []\n",
    "    end_indexes = []\n",
    "\n",
    "    # Iterate through the list to find starting points\n",
    "    for i in range(len(l) - 2):\n",
    "        # Check if the current and next elements form the start sequence\n",
    "        if l[i] == 151644 and l[i+1] == 77091 and l[i+2] == 198:\n",
    "            start_indexes.append(i+3)\n",
    "            # Now look for the first 151645 and 198 after the start\n",
    "            for j in range(i+3, len(l)-1):\n",
    "                if l[j] == 151645 and l[j+1] == 198:\n",
    "                    end_indexes.append(j+2) # **NOTE** the <|im_end|>\\n 2 tokens should be included in the label, so that model can predicate end of output.\n",
    "                    break  # Move to the next start after finding the end\n",
    "\n",
    "    return list(zip(start_indexes, end_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06bbc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn= QwenDataCollator(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57afc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_from_coll=collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3290304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw', 'labels'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_from_coll.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0c4bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an expert radiologist trained in interpreting chest X-rays. Given radiographic image of the chest (frontal view), generate a detailed and clinically accurate radiology report. The report should include a summary of the observed findings and, if possible, an impression that highlights key diagnoses or abnormalities. Do not speculate beyond the image content. Use professional radiological language. Maintain a neutral, factual tone suitable for inclusion in a patient medical record. Do NOT reference previous imaging or mention paging physicians. Instead, describe the chest x-ray in a stand-alone format.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "FINDINGS:\n",
      "None\n",
      "\n",
      "IMPRESSION:\n",
      "The dual-channel ICD device with leads in the right atrium and apex of the right ventricle is visible without any apparent abnormalities. The cardiac silhouette is within normal limits, and there is no evidence of vascular congestion, pleural effusion, or acute focal pneumonia.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.decode(batch_from_coll[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c25de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=batch_from_coll['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb882687",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=labels.tolist()\n",
    "lbl=[x for x in lbl[1] if x != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9079751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINDINGS:\n",
      "None\n",
      "\n",
      "IMPRESSION:\n",
      "The dual-channel ICD device with leads in the right atrium and apex of the right ventricle is visible without any apparent abnormalities. The cardiac silhouette is within normal limits, and there is no evidence of vascular congestion, pleural effusion, or acute focal pneumonia.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.decode(lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "237c35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dfd36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(SFTTrainer):\n",
    "    def custom_loss(self, model, inputs, return_outputs=False,num_items_in_batch=None):\n",
    "        \n",
    "        output=model(**inputs)\n",
    "        loss=output.loss\n",
    "        return (loss,output) if return_outputs else loss\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "746ead9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f78f45c68d349e6baa7290c3cae5d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "985f8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"test\",  # Directory to save the model\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    per_device_train_batch_size=2,  # Batch size for training\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=20,  # Steps interval for saving\n",
    "    # Mixed precision and gradient settings\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    report_to=None,  # Reporting tool for tracking metrics\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    "    label_names=[\"labels\"],  # Names of label columns in dataset\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    processing_class=processor.tokenizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fed]",
   "language": "python",
   "name": "conda-env-.conda-fed-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
